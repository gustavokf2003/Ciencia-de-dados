{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Variavel para escolher o escopo completo 2017-2024 Km 100 a 239 com Km da PRF ou escopo completo 2017-2024 Km 100 a 239 com Km da ANTT\n",
    "escopo = 'completo_antt'  # 'completo' ou 'completo_antt'\n",
    "\n",
    "if escopo == 'completo':\n",
    "    df_train = pd.read_csv('data/escopo_completo/df_train_antt.csv')\n",
    "    df_test = pd.read_csv('data/escopo_completo/df_test_antt.csv')\n",
    "elif escopo == 'completo_antt':\n",
    "    df_train = pd.read_csv('data/escopo_completo_antt/df_train_dnit.parquet')\n",
    "    df_test = pd.read_csv('data/escopo_completo_antt/df_test_dnit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "traducao = {\n",
    "    \"temperature_2m\": \"Temperatura\",\n",
    "    \"relative_humidity_2m\": \"Umidade Relativa\",\n",
    "    \"dew_point_2m\": \"Ponto de Orvalho\",\n",
    "    \"apparent_temperature\": \"Temperatura Aparente\",\n",
    "    \"rain\": \"Precipitação\",\n",
    "    \"wind_speed_10m\": \"Velocidade do Vento\",\n",
    "    \"cloud_cover\": \"Cobertura de Nuvens\",\n",
    "    \"wind_gusts_10m\": \"Rajadas de Vento\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir o tema personalizado\n",
    "sns.set_theme(        \n",
    "    palette=\"dark\",              \n",
    "    font='sans-serif',                \n",
    "    rc={\"axes.spines.right\": False,  \n",
    "        \"axes.spines.top\": False,    \n",
    "        \"figure.figsize\": (10, 6),   \n",
    "        \"axes.facecolor\": \"#ede9e5\",\n",
    "        \"figure.facecolor\": \"#ede9e5\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Aumentar a resolução das figuras\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "numericas = {\"temperature_2m\": \"°C\", \n",
    "             \"relative_humidity_2m\": \"%\",\n",
    "             \"dew_point_2m\": \"°C\",\n",
    "             \"apparent_temperature\": \"°C\",\n",
    "             \"rain\": \"mm\",\n",
    "             \"wind_speed_10m\": \"m/s\",\n",
    "             \"wind_gusts_10m\": \"m/s\",\n",
    "             \"cloud_cover\": \"%\"}\n",
    "\n",
    "if escopo != 'completo':\n",
    "    for coluna, unidade in numericas.items():\n",
    "        sns.regplot(x=coluna, y='acidente', data=df_train, scatter=False)\n",
    "        plt.title(f'Relação entre {traducao[coluna]} e acidentes')\n",
    "        plt.xlabel(f'{traducao[coluna]} ({unidade})')\n",
    "        plt.ylabel('Proporção de acidentes')\n",
    "        plt.yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plotar um histograma para a variável 'km'\n",
    "numericas = ['km']\n",
    "for coluna in numericas: \n",
    "    acidente_0 = df_train[df_train['acidente'] == 0][coluna].value_counts()\n",
    "    acidente_1 = df_train[df_train['acidente'] == 1][coluna].value_counts()\n",
    "    # Junta em um DataFrame\n",
    "    df_plot = pd.DataFrame({\n",
    "        'Não Acidente': acidente_0,\n",
    "        'Acidente': acidente_1\n",
    "    })\n",
    "\n",
    "    df_plot.plot(kind='area')\n",
    "\n",
    "    plt.title(f'Histograma da variável {coluna}')\n",
    "    plt.xlabel(coluna)\n",
    "    plt.ylabel('Frequêcia')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lista de variáveis categóricas\n",
    "if escopo != 'completo':\n",
    "    categoricas = ['ano','mes', 'dia', 'hora', 'fim_semana', 'dia_da_semana', 'feriado_br', \n",
    "                   'municipio', 'sentido_via', 'tipo_tracado', 'tipo_de_uso_do_solo',\n",
    "                   'tipo_pavimento', 'tipo_perfil_de_terreno', 'marginal', 'iluminacao', \n",
    "                   'velocidade_regulamentada_veiculos_leves','velocidade_regulamentada_veiculos_pesados', \n",
    "                   'numero_de_faixas','weather_code']\n",
    "else:\n",
    "    categoricas = ['ano','mes', 'dia', 'hora', 'fim_semana', 'dia_da_semana', 'feriado_br', \n",
    "                   'municipio', 'sentido_via', 'tipo_tracado', 'tipo_de_uso_do_solo',\n",
    "                   'tipo_pavimento', 'tipo_perfil_de_terreno', 'marginal', 'iluminacao', \n",
    "                   'velocidade_regulamentada_veiculos_leves','velocidade_regulamentada_veiculos_pesados', \n",
    "                   'numero_de_faixas']\n",
    "\n",
    "for coluna in categoricas:\n",
    "    if coluna not in [\"municipio\", 'weather_code']:\n",
    "        acidente_0 = df_train[df_train['acidente'] == 0][coluna].value_counts().sort_index()\n",
    "        acidente_1 = df_train[df_train['acidente'] == 1][coluna].value_counts().sort_index()\n",
    "    acidente_0 = df_train[df_train['acidente'] == 0][coluna].value_counts()\n",
    "    acidente_1 = df_train[df_train['acidente'] == 1][coluna].value_counts()\n",
    "    # Junta em um DataFrame\n",
    "    df_plot = pd.DataFrame({\n",
    "        'Não Acidente': acidente_0,\n",
    "        'Acidente': acidente_1\n",
    "    })\n",
    "\n",
    "    # Faz o gráfico de barras lado a lado\n",
    "    df_plot.plot(kind='bar', width=0.8)\n",
    "\n",
    "    plt.title(f'Frêquencia de Acidente vs Não Acidente para {coluna}')\n",
    "    plt.xlabel(coluna)\n",
    "    plt.ylabel('Frequêcia')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preparar os dados para treinamento\n",
    "if escopo == 'completo':\n",
    "    categoricas = ['km', 'sentido_via', 'tipo_tracado', 'tipo_perfil_de_terreno', 'tipo_pavimento',\n",
    "       'tipo_de_uso_do_solo', 'numero_de_faixas', 'municipio','velocidade_regulamentada_veiculos_leves',\n",
    "       'velocidade_regulamentada_veiculos_pesados', 'marginal', 'iluminacao',\n",
    "       'mes', 'dia', 'hora', 'dia_da_semana', 'fim_semana',\n",
    "       'feriado_br']\n",
    "    \n",
    "    features = ['km', 'sentido_via', 'tipo_tracado', 'tipo_perfil_de_terreno', 'tipo_pavimento',\n",
    "       'tipo_de_uso_do_solo', 'numero_de_faixas', 'municipio','velocidade_regulamentada_veiculos_leves',\n",
    "       'velocidade_regulamentada_veiculos_pesados', 'marginal', 'iluminacao',\n",
    "       'mes', 'dia', 'hora', 'ano', 'dia_da_semana', 'fim_semana',\n",
    "       'feriado_br']\n",
    "elif escopo == 'completo_antt':\n",
    "    categoricas = ['km', 'sentido_via', 'tipo_tracado', 'tipo_perfil_de_terreno', 'tipo_pavimento',\n",
    "       'tipo_de_uso_do_solo', 'numero_de_faixas', 'municipio','velocidade_regulamentada_veiculos_leves',\n",
    "       'velocidade_regulamentada_veiculos_pesados', 'marginal', 'iluminacao',\n",
    "       'mes', 'dia', 'hora', 'dia_da_semana', 'fim_semana', 'feriado_br']\n",
    "    \n",
    "    features = ['km', 'sentido_via', 'tipo_tracado', 'tipo_perfil_de_terreno', 'tipo_pavimento',\n",
    "       'tipo_de_uso_do_solo', 'numero_de_faixas', 'municipio','velocidade_regulamentada_veiculos_leves',\n",
    "       'velocidade_regulamentada_veiculos_pesados', 'marginal', 'iluminacao',\n",
    "       'mes', 'dia', 'hora', 'ano', 'dia_da_semana', 'fim_semana',\n",
    "       'feriado_br', 'temperature_2m', 'relative_humidity_2m',\n",
    "       'dew_point_2m', 'apparent_temperature', 'rain', 'wind_speed_10m',\n",
    "       'weather_code', 'cloud_cover', 'wind_gusts_10m', 'A','B','C','D','E','F','G','H','I','J']\n",
    "\n",
    "\n",
    "# Concatenar X_train e X_test para garantir que ambas as matrizes de características tenham as mesmas colunas\n",
    "X = pd.concat([df_train[features], df_test[features]])\n",
    "X = pd.get_dummies(X, columns=categoricas, drop_first=True)\n",
    "\n",
    "# Identificar colunas numéricas\n",
    "colunas_numericas = list(set(features) - set(categoricas))\n",
    "\n",
    "# Padronizar os dados numéricos\n",
    "scaler = StandardScaler()\n",
    "X[colunas_numericas] = scaler.fit_transform(X[colunas_numericas])\n",
    "\n",
    "# Separar novamente em X_train e X_test\n",
    "X_train = X.iloc[:len(df_train)]\n",
    "X_test = X.iloc[len(df_train):]\n",
    "\n",
    "y_train = df_train['acidente']\n",
    "y_test = df_test['acidente']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from cuml.svm import SVC  # SVM da cuML (GPU)\n",
    "from cuml.ensemble import RandomForestClassifier  # Random Forest da cuML\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "# Definir o número de folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "param_grids = {\n",
    "   \"SVM\": {\n",
    "        \"C\": [1, 2, 5],\n",
    "        \"kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        \"gamma\": ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "params_grids = {\n",
    "    \"MLP\": {\n",
    "        \"hidden_layer_sizes\": [\n",
    "            (512, 256, 128, 64),\n",
    "            (256, 128, 64),\n",
    "            (128, 64),\n",
    "        ],\n",
    "        \"activation\": [\"relu\", \"tanh\"],\n",
    "        \"lr\": [0.0001],\n",
    "        \"optimizer\": [\"adam\"],\n",
    "        \"dropout_rate\": [0.0, 0.1],\n",
    "        \"weight_decay\": [0.0, 1e-4],\n",
    "    },\n",
    "    \"RF\": {\n",
    "        \"n_estimators\": [100, 200, 500],\n",
    "        \"split_criterion\": ['gini', 'entropy'],\n",
    "        'bootstrap': [True, False],\n",
    "        \"max_depth\": [32, 64]\n",
    "    },\n",
    "   \"SVM\": {\n",
    "        \"C\": [1, 2, 5],\n",
    "        \"kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        \"gamma\": ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Criar dicionário para armazenar resultados\n",
    "results = {\"SVM\": [], \"RF\": [], \"MLP\": []}\n",
    "\n",
    "# Converter os dados para GPU\n",
    "X_train_gpu = cp.array(X_train.values.astype('float32'))\n",
    "y_train_gpu = cp.array(y_train.values.astype('int32'))\n",
    "\n",
    "\n",
    "# Definição do modelo MLP usando PyTorch (GPU)\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, activation=\"relu\", dropout_rate=0.0):\n",
    "        super(MLPModel, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Seleciona a função de ativação\n",
    "        if activation == \"relu\":\n",
    "            act_fn = nn.ReLU()\n",
    "        elif activation == \"tanh\":\n",
    "            act_fn = nn.Tanh()\n",
    "        elif activation == \"leaky_relu\":\n",
    "            act_fn = nn.LeakyReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"A função de ativação '{activation}' não é suportada.\")\n",
    "\n",
    "        # Camadas ocultas com dropout\n",
    "        for size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, size))\n",
    "            layers.append(act_fn)\n",
    "            if dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = size\n",
    "\n",
    "        # Camada de saída\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "round_ = 0\n",
    "# Loop pelos modelos\n",
    "for model_name, param_grid in param_grids.items():\n",
    "    # Loop pelos hiperparâmetros\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(f\"\\nTreinando {model_name} {round_}\")\n",
    "        round_ += 1\n",
    "        recall_list, specificity_list, auc_list = [], [], []\n",
    "\n",
    "        # Loop pelos folds\n",
    "        for train_idx, val_idx in kf.split(X_train):\n",
    "            X_tr, X_val = X_train_gpu[train_idx], X_train_gpu[val_idx]\n",
    "            y_tr, y_val = y_train_gpu[train_idx], y_train_gpu[val_idx]\n",
    "\n",
    "            # Criar e treinar o modelo na GPU\n",
    "            if model_name == \"SVM\":\n",
    "                if params[\"kernel\"] == 'linear':\n",
    "                    model = LinearSVC(C=params[\"C\"], max_iter=10000)\n",
    "                    model.fit(X_train, y_train)\n",
    "\n",
    "                    y_pred = model.predict(X_val)\n",
    "                else:\n",
    "                    model = SVC(C=params[\"C\"], kernel=params[\"kernel\"], gamma=params[\"gamma\"])\n",
    "                    model.fit(X_tr, y_tr)\n",
    "                \n",
    "                    # Obter as distâncias da margem de decisão\n",
    "                    decision_scores = model.decision_function(X_val)\n",
    "                \n",
    "                    # Aplicar o threshold personalizado\n",
    "                    y_pred = (decision_scores >  0.5).astype(int)\n",
    "\n",
    "\n",
    "            elif model_name == \"RF\":\n",
    "                model = RandomForestClassifier(\n",
    "                    n_estimators=params[\"n_estimators\"], \n",
    "                    max_depth=params[\"max_depth\"],\n",
    "                    split_criterion=params['split_criterion'],\n",
    "                    bootstrap=params['bootstrap']\n",
    "                )\n",
    "                model.fit(X_tr, y_tr)\n",
    "            \n",
    "                # Obter probabilidades e aplicar threshold\n",
    "                y_prob = model.predict_proba(X_val)[:, 1]\n",
    "                y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "\n",
    "            elif model_name == \"MLP\":\n",
    "                input_size = X_train.shape[1]\n",
    "                model = MLPModel(input_size, params['hidden_layer_sizes'], params['activation'], params['dropout_rate']).cuda()\n",
    "                if params['optimizer'] == 'adam':\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "                else:\n",
    "                    optimizer = optim.SGD(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "                    \n",
    "                criterion = nn.BCELoss()  \n",
    "                \n",
    "                # Converter dados para PyTorch\n",
    "                X_tr_torch = torch.tensor(X_tr.get(), dtype=torch.float32).cuda()\n",
    "                y_tr_torch = torch.tensor(y_tr.get(), dtype=torch.float32).cuda().unsqueeze(1)\n",
    "            \n",
    "                X_val_torch = torch.tensor(X_val.get(), dtype=torch.float32).cuda()\n",
    "            \n",
    "                # Treinamento\n",
    "                model.train()\n",
    "                for _ in range(400):  \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(X_tr_torch)\n",
    "                    loss = criterion(outputs, y_tr_torch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "                # Previsões com threshold personalizado\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    y_prob = model(X_val_torch).cpu().numpy()\n",
    "                    y_pred = (y_prob >  0.5).astype(int)\n",
    "\n",
    "                del X_tr_torch, y_tr_torch, X_val_torch\n",
    "\n",
    "            # Calcular métricas\n",
    "            y_val_cpu = y_val.get() if hasattr(y_val, \"get\") else (y_val.cpu().numpy() if hasattr(y_val, \"cpu\") else y_val)\n",
    "            y_pred_cpu = y_pred.get() if model_name != \"MLP\" else y_pred\n",
    "            tn, fp, fn, tp = confusion_matrix(y_val_cpu, y_pred_cpu).ravel()\n",
    "\n",
    "            sensitivity = tp / (tp + fn + 1e-8)\n",
    "            specificity = tn / (tn + fp + 1e-8)\n",
    "            auc = roc_auc_score(y_val_cpu, y_pred_cpu)\n",
    "\n",
    "            recall_list.append(sensitivity)\n",
    "            specificity_list.append(specificity)\n",
    "            auc_list.append(auc)\n",
    "\n",
    "\n",
    "        # Salvar resultados médios para essa configuração de hiperparâmetros\n",
    "        results[model_name].append({\n",
    "            **params,\n",
    "            'specificity': specificity_list,\n",
    "            'recall': recall_list,\n",
    "            'auc': auc_list,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calc_score(x):\n",
    "    return np.mean(x['auc']) + np.mean(x['specificity']) + np.mean(x['recall']) * 1.5\n",
    "\n",
    "for model_name, hyperparams_list in results.items():\n",
    "    if hyperparams_list:\n",
    "        scored_models = [(model, calc_score(model)) for model in hyperparams_list]\n",
    "        best_model, _ = max(scored_models, key=lambda t: t[1])\n",
    "        \n",
    "        print(f\"\\nMelhores Hiperparâmetros para {model_name}:\")\n",
    "        print(best_model)\n",
    "        for metric in ['specificity', 'recall', 'auc']:\n",
    "            media = np.mean(best_model[metric])\n",
    "            desvio = np.std(best_model[metric])\n",
    "            print(f\"{metric}: média = {media:.4f} ± {desvio:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\nNenhum resultado para {model_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, activation=\"relu\", dropout_rate=0.0):\n",
    "        super(MLPModel, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Seleciona a função de ativação\n",
    "        if activation == \"relu\":\n",
    "            act_fn = nn.ReLU()\n",
    "        elif activation == \"tanh\":\n",
    "            act_fn = nn.Tanh()\n",
    "        elif activation == \"leaky_relu\":\n",
    "            act_fn = nn.LeakyReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"A função de ativação '{activation}' não é suportada.\")\n",
    "\n",
    "        # Camadas ocultas com dropout\n",
    "        for size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, size))\n",
    "            layers.append(act_fn)\n",
    "            if dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = size\n",
    "\n",
    "        # Camada de saída\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    \"MLP\": {'activation': 'relu', 'dropout_rate': 0.1, 'hidden_layer_sizes': (256, 128, 64), 'lr': 0.0001, 'optimizer': 'adam', 'weight_decay': 0.0\n",
    "    },\n",
    "}\n",
    "\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Converta os dados para tensores e mande para o dispositivo correto\n",
    "X_train_tensor = torch.tensor(X_train.astype('float32').values, dtype=torch.float32).to('cpu')\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to('cpu')\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.astype('float32').values, dtype=torch.float32).to('cpu')\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to('cpu')\n",
    "\n",
    "# ---------------------- MLP Teste (já existente) ----------------------\n",
    "input_size = X_train.shape[1]\n",
    "model = MLPModel(\n",
    "    input_size, \n",
    "    best_params['MLP']['hidden_layer_sizes'], \n",
    "    best_params['MLP']['activation'], \n",
    "    best_params['MLP']['dropout_rate']\n",
    ")\n",
    "\n",
    "# Enviar modelo para GPU se disponível\n",
    "model = model.to('cpu')\n",
    "\n",
    "if best_params[\"MLP\"]['optimizer'] == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params[\"MLP\"]['lr'], weight_decay=best_params[\"MLP\"]['weight_decay'])\n",
    "else:\n",
    "    optimizer = optim.SGD(model.parameters(), lr=best_params[\"MLP\"]['lr'], weight_decay=best_params[\"MLP\"]['weight_decay'])\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model.train()\n",
    "for epochs in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_tensor)\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  y_prob = model(X_test_tensor).numpy()\n",
    "  y_pred = (y_prob >  0.5).astype(int)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_tensor, y_pred).ravel()\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "sensitivity = tp / (tp + fn + 1e-8)\n",
    "specificity = tn / (tn + fp + 1e-8)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "print(\"\\nMLP - Métricas no conjunto de teste:\")\n",
    "print(f\"Acuracia: {accuracy:.4f}\")\n",
    "print(f\"Especificidade: {specificity:.4f}\")\n",
    "print(f\"Sensibilidade (Recall): {sensitivity:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "print(\"\\nAvaliando SVM nos dados de teste...\")\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC(C=1.0, max_iter=10000)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "sensitivity = tp / (tp + fn + 1e-8)\n",
    "specificity = tn / (tn + fp + 1e-8)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nSVM (cuML) - Métricas no conjunto de teste:\")\n",
    "print(f\"Especificidade: {specificity:.4f}\")\n",
    "print(f\"Sensibilidade (Recall): {sensitivity:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#import cupy as cp\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest da cuML\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# ---------------------- Random Forest com GPU (cuML) ----------------------\n",
    "print(\"\\nAvaliando Random Forest nos dados de teste...\")\n",
    "\n",
    "\n",
    "rf_best = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    criterion='entropy',\n",
    "    bootstrap=False,\n",
    "    max_depth=64,\n",
    ")\n",
    "\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "#y_prob_rf = predict_in_batches(model, X_test.values, batch_size=4096)\n",
    "y_prob = rf_best.predict_proba(X_test)[:, 1]\n",
    "y_pred_rf = (y_prob > 0.5).astype(int)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_rf).ravel()\n",
    "sensitivity = tp / (tp + fn + 1e-8)\n",
    "specificity = tn / (tn + fp + 1e-8)\n",
    "auc = roc_auc_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\nRandom Forest (cuML) - Métricas no conjunto de teste:\")\n",
    "print(f\"Especificidade: {specificity:.4f}\")\n",
    "print(f\"Sensibilidade (Recall): {sensitivity:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tn, fp, fn, tp"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 7707920,
     "sourceId": 12376315,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
